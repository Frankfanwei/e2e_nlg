{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hands On\n",
    "\n",
    "Now let's get our hands dirty. We show how the whole training pipeline works and then we also look at how to write a custom Keras Layer for the SC-LSTM cell. You can donwload the whole codebase here: [Cool Code Here](https://github.com/jderiu/e2e_nlg) \n",
    "\n",
    "## Preprocessing\n",
    "The first step in every machine learning pipeline is the preprocessing. The preprocessing consists of the following steps:\n",
    "- Delexicalizing the data: Replacing the names of the restaurant by placeholders. \n",
    "- Vectorizing the data: translating the meaning represenations into binarized vectors as well as transforming the utterances in a list of indices, each character is represented by an index from the vocabulary (i.e. char2idx mapping).\n",
    "- Extracting the syntactic information: get the first word of the utterance and the follow-up sentences and encode those into a binary vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-13 11:21:13,760 : INFO : Base directory:C:\\Users\\deri\\Documents\\Git Projects\\e2e_nlg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of Trainset: 42061 Validationset: 4672 Testset: 4693\n",
      "Parsed MR:\n",
      "{'area': 'riverside',\n",
      " 'customer rating': '5 out of 5',\n",
      " 'eatType': 'coffee shop',\n",
      " 'food': 'Japanese',\n",
      " 'name': 'The Golden Palace',\n",
      " 'priceRange': 'more than £30'}\n",
      "Original Output:  The coffee shop The Golden Palace is north of the city centre. It serves expensive food and has a 5 star rating.\n",
      "Delexicalized Output:  The coffee shop XNAMEX is north of the city centre. It serves expensive food and has a 5 star rating.\n",
      "{'area': 'XAREAX',\n",
      " 'customer rating': 'XCUSTX',\n",
      " 'eatType': 'XEATX',\n",
      " 'familyFriendly': 'XFAMX',\n",
      " 'food': 'XFOODX',\n",
      " 'name': 'XNAMEX',\n",
      " 'near': 'XNESRX',\n",
      " 'priceRange': 'XPRICX'}\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os, sys\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(width=41, compact=True)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#makes sure that the modules can be loaded\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "nb_dir = nb_dir.replace('\\\\src', '')\n",
    "sys.path.append(nb_dir)\n",
    "logging.info('Base directory:' + nb_dir)\n",
    "## should output: /some-path/e2e_nlg\n",
    "\n",
    "from src.data_processing.delexicalise_data import _delex_nlg_data, _retrieve_mr_ontology, _load_attributes\n",
    "data_path = os.path.join(nb_dir, 'data/e2e_nlg')\n",
    "# List of what attributes we want to replace with a placeholder\n",
    "delex_attributes = [\"name\", \"near\", \"food\"]\n",
    "# File name for the table of attribute-placeholder pairs\n",
    "attribute_fname = 'ontology/attribute_tags.txt'\n",
    "attribute_tokens = _load_attributes(os.path.join(data_path, attribute_fname))\n",
    "\n",
    "train_delex = _delex_nlg_data('trainset.csv', data_path, delex_attributes, attribute_tokens)\n",
    "valid_delex = _delex_nlg_data('devset.csv', data_path, delex_attributes, attribute_tokens)\n",
    "test_delex = _delex_nlg_data('testset.csv', data_path, delex_attributes, attribute_tokens)\n",
    "\n",
    "print('Lengths of Trainset: {} Validationset: {} Testset: {}'.format(\n",
    "    len(train_delex['mr_raw']), \n",
    "    len(valid_delex['mr_raw']), \n",
    "    len(test_delex['mr_raw'])))\n",
    "\n",
    "#Print an example\n",
    "idx = 110 #(change me)\n",
    "print('Parsed MR:')\n",
    "pp.pprint(train_delex['parsed_mrs'][idx])\n",
    "print('Original Output: ', train_delex['outputs_raw'][idx])\n",
    "print('Delexicalized Output: ', train_delex['delexicalised_texts'][idx])\n",
    "pp.pprint(attribute_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to extract the data ontology, which we need to vectorize the data later. The ontoloty is a dictonary of values2idx vocabulaires. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of attributes:\n",
      "['name', 'eatType', 'priceRange',\n",
      " 'customer rating', 'near', 'food',\n",
      " 'area', 'familyFriendly']\n",
      "Value2Idx Vocabulary for priceRange:\n",
      "{'cheap': 0,\n",
      " 'high': 1,\n",
      " 'less than £20': 2,\n",
      " 'moderate': 3,\n",
      " 'more than £30': 4,\n",
      " '£20-25': 5}\n"
     ]
    }
   ],
   "source": [
    "full_mr_list = train_delex['parsed_mrs'] + valid_delex['parsed_mrs'] + test_delex['parsed_mrs']\n",
    "mr_data_ontology = _retrieve_mr_ontology(full_mr_list)\n",
    "print('List of attributes:')\n",
    "pp.pprint(list(mr_data_ontology.keys()))\n",
    "print('Value2Idx Vocabulary for priceRange:')\n",
    "pp.pprint(mr_data_ontology['priceRange'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "Next, we transform the preprocessed data into vectors, which can be interpreted by our neural network. This requires the following steps:\n",
    "- Transform the meaning representations into a binary representation. For this, we rely on the ontology we extracted in the cell above.\n",
    "- Transform the utterances into a list of indices, which are then given as input to the neural network. Each index corresponds to a alphanumeric character.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Meaning Representations\n",
    "For each attribute, we create a one-hot encoded vector, which indicates which value is present in the utterance. We add am extra dimension to the vectors for those cases where the attrbute is missing. Note that the delexicalized attributes only have lenghts of two. This is just to indicate if the attribute is present or not, since the value is replaced by a placeholder.\n",
    "\n",
    "We frist take the processing of one MR apart and then we process the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'area': 3,\n",
      " 'customer rating': 7,\n",
      " 'eatType': 4,\n",
      " 'familyFriendly': 3,\n",
      " 'food': 2,\n",
      " 'name': 2,\n",
      " 'near': 2,\n",
      " 'priceRange': 7}\n"
     ]
    }
   ],
   "source": [
    "from src.data_processing.vectorize_data import _compute_vector_length, _vectorize_single_mr\n",
    "\n",
    "# First compute the length of the one-hot encoded vectors:\n",
    "vector_lengts = _compute_vector_length(mr_data_ontology, delex_attributes)\n",
    "pp.pprint(vector_lengts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'more than £30'\n",
      "array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [1.],\n",
      "       [0.],\n",
      "       [0.]])\n"
     ]
    }
   ],
   "source": [
    "#Process one meaning representation:\n",
    "mr = train_delex['parsed_mrs'][idx]\n",
    "vec = _vectorize_single_mr(mr, mr_data_ontology, vector_lengts, delex_attributes)\n",
    "pp.pprint(train_delex['parsed_mrs'][idx]['priceRange'])\n",
    "pp.pprint(vec['priceRange'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the meaning representation vectorization over the whole dataset. We store the result in a dictionary of attribute name to vectors. Each row corresponds to one datapoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: (42061, 7)\n"
     ]
    }
   ],
   "source": [
    "# Vectorize meaning representations\n",
    "from src.data_processing.vectorize_data import _vectorize_mrs\n",
    "\n",
    "train_mr_vecs = _vectorize_mrs(train_delex['parsed_mrs'], mr_data_ontology, delex_attributes)\n",
    "valid_mr_vecs = _vectorize_mrs(valid_delex['parsed_mrs'], mr_data_ontology, delex_attributes)\n",
    "test_mr_vecs = _vectorize_mrs(test_delex['parsed_mrs'], mr_data_ontology, delex_attributes)\n",
    "print('Dimensions: {}'.format(train_mr_vecs['priceRange'].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Utterances\n",
    "Now we just need to create the representation for the utterances. For this load the vocabulary and then just apply the transforamtion. One important detail: since Keras works with fixed lenght sequences, we need to pad the texts (or cut them off) so that all the vectors have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Len: 68\n",
      "Idx of character \"a\": 5\n",
      "Dummy Idx: 68 Unknown Idx: 69\n",
      "Input Data Shape: (42061, 256)\n"
     ]
    }
   ],
   "source": [
    "#Step 1 Load Vocabulary\n",
    "import json\n",
    "from src.data_processing.utils import convert2indices \n",
    "\n",
    "char_fname = open(os.path.join(data_path, 'vocabulary.json'), 'rt', encoding='utf-8')\n",
    "char_vocab = json.load(char_fname)\n",
    "print('Vocab Len: {}'.format(len(char_vocab)))\n",
    "print('Idx of character \"a\": {}'.format(char_vocab['a']))\n",
    "\n",
    "#Always use a dummy character for padding and a unk character for unknown tokens (or characters in this case)\n",
    "dummy_char = max(char_vocab.values()) + 1\n",
    "unk_char = max(char_vocab.values()) + 2\n",
    "\n",
    "print('Dummy Idx: {} Unknown Idx: {}'.format(dummy_char, unk_char))\n",
    "\n",
    "#Step 2 Convert to Indices\n",
    "max_sentence_len = 256\n",
    "\n",
    "train_idx_data = convert2indices(train_delex['delexicalised_texts'], char_vocab, dummy_char, unk_char, max_sentence_len)\n",
    "valid_idx_data = convert2indices(valid_delex['delexicalised_texts'], char_vocab, dummy_char, unk_char, max_sentence_len)\n",
    "test_idx_data = convert2indices(test_delex['delexicalised_texts'], char_vocab, dummy_char, unk_char, max_sentence_len)\n",
    "\n",
    "#The shape is number of datapoints x sentence length\n",
    "print('Input Data Shape: {}'.format(train_idx_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Word Features\n",
    "Next we prepare the syntactic features, which we use for to add more variety to the generated utterances. For sake of brievety, we only show the extraction of the first word features. The other two manipulations are present in the full code version on Github. \n",
    "\n",
    "The extraciton of the first word is done in following steps:\n",
    "- Word tokenize all delexicalized utterances.\n",
    "- Extract the first word of each utterance. \n",
    "- Create a vocabulary of first words, i.e. first word-to-idx mapping. We only keep first words, which appear at least 100 times. Otherwise the neural network has difficulties learning the correlation between the first word and the utterance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'coffee', 'shop', 'XNAMEX', 'is', 'north', 'of', 'the', 'city', 'centre', '.'], ['It', 'serves', 'expensive', 'food', 'and', 'has', 'a', '5', 'star', 'rating', '.']]\n",
      "Mapping from First Word 2 Index:\n",
      "[('XNAMEX', 0), ('Located', 1),\n",
      " ('For', 2), ('In', 3), ('A', 4),\n",
      " ('XNESRX', 5), ('An', 6), ('Near', 7),\n",
      " ('There', 8), ('On', 9), ('XFOODX', 10),\n",
      " ('The', 11), ('With', 12),\n",
      " ('Serving', 13), ('If', 14), ('At', 15),\n",
      " ('Riverside', 16), ('By', 17),\n",
      " ('You', 18), ('Family', 19)]\n",
      "Shape of First words: (42061, 21)\n",
      "The word \"The\" corresponds to : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Tokenize the Utterances\n",
    "from src.data_processing.surface_feature_vectors import _sentence_tok, _utterance_first_word_vocab, _utt_fw_features\n",
    "\n",
    "train_tok = _sentence_tok(train_delex['delexicalised_texts'])\n",
    "valid_tok = _sentence_tok(valid_delex['delexicalised_texts'])\n",
    "test_tok = _sentence_tok(test_delex['delexicalised_texts'])\n",
    "\n",
    "#Print an example: Note that we both tokenize on sentence and word level. The sentence level tokeniztion can be used for other manipulations.\n",
    "print(train_tok[idx])\n",
    "\n",
    "#Step 2: Generate fw2idx mapping\n",
    "utt_fw_vocab = _utterance_first_word_vocab(train_tok + valid_tok + test_tok, min_freq=100)\n",
    "inverse_utt_fw_vocab = {v: k for k, v in utt_fw_vocab.items()}\n",
    "print('Mapping from First Word 2 Index:')\n",
    "pp.pprint(list(utt_fw_vocab.items()))\n",
    "\n",
    "#Step 3 Create Surface Level Features\n",
    "train_utt_fw = _utt_fw_features(train_tok, utt_fw_vocab)\n",
    "valid_utt_fw = _utt_fw_features(valid_tok, utt_fw_vocab)\n",
    "test_utt_fw = _utt_fw_features(test_tok, utt_fw_vocab)\n",
    "\n",
    "utt_fw_input_dimension = train_utt_fw.shape[1]\n",
    "print('Shape of First words: {}'.format(train_utt_fw.shape))\n",
    "print('The word \"{}\" corresponds to : {}'.format(train_tok[idx][0][0], train_utt_fw[idx]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WAIT A MOMENT: THAT'S CHEATING !!!!\n",
    "Of course, we do nat have access to the correct first word during test time. This is indeed a major drawback of this approach. The solution is to sample n different first words for each meaning representation during test time. This then corresponds to n different utterances. Then we have to rank those utterances according to their semantic correctness, as there are conficlting combinations of meaning represenations and first words. For instance, when there is no location mentioned but the first word is \"Located\". \n",
    "\n",
    "So let's sample 10 different first words for each meaning representation in the test set. We have an extra test set which contains only the meaning representations (i.e. no reference utterances given)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of Test Set: 630\n",
      "(630, 2)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "import numpy as np\n",
    "import random\n",
    "from src.data_processing.generate_evaluation_data import _read_data, _parse_raw_mr\n",
    "test_mr_only = os.path.join(data_path, 'test_mr_only.csv')\n",
    "\n",
    "#Read the MR only test set \n",
    "test_mr_only_raw = _read_data(test_mr_only)\n",
    "test_process_mr_only = _parse_raw_mr(test_mr_only_raw)\n",
    "test_vectorised_mrs_only = _vectorize_mrs(test_process_mr_only, mr_data_ontology, delex_attributes)\n",
    "test_mr_only_dummy_idx = np.zeros(shape=(len(test_mr_only_raw), max_sentence_len)) #dummy output idx\n",
    "\n",
    "def sample_utt_fw_for_mr(nsamples):\n",
    "    utt_fw_samples = random.sample(list(utt_fw_vocab.values()), k=nsamples)\n",
    "    dummy_idx = max(utt_fw_vocab.values()) + 1\n",
    "    utt_fw_vec = []\n",
    "    for fidx in utt_fw_samples:\n",
    "        v = np.zeros(shape=(dummy_idx + 1, 1))\n",
    "        v[fidx] = 1.0\n",
    "        utt_fw_vec.append(v)\n",
    "    return utt_fw_vec\n",
    "    \n",
    "first_word_features = []    \n",
    "for mr in test_process_mr_only:\n",
    "    utt_fw_vec = sample_utt_fw_for_mr(10)\n",
    "    first_word_features.append(utt_fw_vec)\n",
    "\n",
    "print('Len of Test Set: {}'.format(len(first_word_features)))\n",
    "print(test_vectorised_mrs_only['name'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally some Deep Learning\n",
    "\n",
    "The preprocessing is finally done, now we can focus on training the semantically conditioned LSTM. For this we build the architecture and then start the training procedure. \n",
    "\n",
    "The architecture is rather simple: \n",
    "- Inputs\n",
    "    - We define inputs for each of the eight possible attribues form the domain ontoloty. \n",
    "    - We define inputs for the syntactic features (in this case, we only look at the first word of the utterance).\n",
    "    - We define the input for the expected output utterance, which is used to perform teacher forcing and to comput the reconstriction loss.\n",
    "- Embeddings. Since we work on the character level, we just use a one-hot representation of the characters. This menans that we represent each character by a vector, where the character index is set to 1.\n",
    "- Generator. We use a custom made SC-LSTM Layer (more on this in the next post), which has two inputs and three outputs:\n",
    "    - Input 1: The meaning representation + syntactic representation input.\n",
    "    - Input 2: The previously generated token. This is important as the generator needs to learn to produce the next character given the current cell state and the previously generated token. For this we just shift the output-utterance by one to the right.\n",
    "    - Outputs: The generated utterance, the last state of the extra cell (meaning representation cell) and the history of all meaning representation states. These outputs are used to compute the loss. $$ F(\\theta) = \\sum_tp_t^Tlog(y_t) + \\left \\| d_T \\right \\| + \\sum_{t=0}^{T-1}\\eta \\xi^{\\left \\| d_t - d_{t-1} \\right \\|} $$ The loss is a combination of the reconstruction loss, the nrom of the last meaning represenation cell state (should be 0) and the average difference between two consecutive cell states (make sure that the cell does not get consumed to quickly).  \n",
    "- Models. Finally two models are defined. \n",
    "    - The training model, which outputs the three losses.\n",
    "    - The test model, which outputs the indices of the characters in the generated utterance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from keras.layers import Lambda, Embedding, Input, concatenate, ZeroPadding1D\n",
    "from src.architectures.custom_layers.sem_recurrent import SC_LSTM\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.models import Model\n",
    "\n",
    "class SCLSTM(object):\n",
    "    def __init__(self, syntactic_manipulation, vocabulary, sample_out_size):\n",
    "        self.optimizer = Adadelta(lr=1, epsilon=1e-8, rho=0.95, decay=0.0001, clipnorm=10)\n",
    "        self.vocabuary = vocabulary\n",
    "        self.sample_out_size = sample_out_size\n",
    "        \n",
    "        self.build_model(syntactic_manipulation)\n",
    "    \n",
    "    def build_model(self, syntactic_manipulation):\n",
    "        #max_sentence_len\n",
    "        #we need 3 extra classes for the dummy-char, unk-char and one for start-char\n",
    "        nclasses = len(self.vocabuary) + 3\n",
    "        lstm_units = 1024\n",
    "        dropout_word_idx = max(self.vocabuary.values()) + 1\n",
    "        \n",
    "        # == == == == == #\n",
    "        # Define Inputs  #\n",
    "        # == == == == == #\n",
    "        \n",
    "        semantic_inputs = []\n",
    "        for attribute in sorted(list(vector_lengts.keys())):\n",
    "            vec_len = vector_lengts[attribute]\n",
    "            attr_idx = Input(batch_shape=(None, vec_len), dtype='float32', name='{}_idx'.format(attribute.replace(' ', '_')))\n",
    "            semantic_inputs.append(attr_idx)\n",
    "        \n",
    "        if syntactic_manipulation:\n",
    "            attr_idx = Input(batch_shape=(None, utt_fw_input_dimension), dtype='float32', name='{}_idx'.format('utt_fw'))\n",
    "            inputs = semantic_inputs + [attr_idx]\n",
    "        else:\n",
    "            inputs = semantic_inputs\n",
    "        \n",
    "        meaning_representation = concatenate(inputs=inputs)\n",
    "        \n",
    "        # == == == == == #\n",
    "        # Define Outputs #\n",
    "        # == == == == == #\n",
    "        \n",
    "        output_idx = Input(batch_shape=(None, self.sample_out_size), dtype='int32', name='character_output')\n",
    "        \n",
    "        #we just represent characters as a one-hot encoded vectors. This makes the computation of the reconstruciton loss easier. \n",
    "        one_hot_weights = np.identity(nclasses)\n",
    "\n",
    "        one_hot_out_embeddings = Embedding(\n",
    "            input_length=self.sample_out_size,\n",
    "            input_dim=nclasses,\n",
    "            output_dim=nclasses,\n",
    "            weights=[one_hot_weights],\n",
    "            trainable=False,\n",
    "            name='one_hot_out_embeddings'\n",
    "        )\n",
    "        \n",
    "        #dimensions: batch_size x max_sentence_len x nclasses\n",
    "        output_one_hot_embeddings = one_hot_out_embeddings(output_idx)\n",
    "        \n",
    "        # == == == == == ==#\n",
    "        # Define Generator #\n",
    "        # == == == == == ==#\n",
    "        \n",
    "        #Step 1: Preprend a start-vector to the inputs, since the generation of the next character is conditioned on the previous.\n",
    "        #Thus, we need to shift the inputs by one. w_i ~ P(w_i | w_(i-1), ... , w_0, d_(i-1))\n",
    "        padding = ZeroPadding1D(padding=(1, 0))(output_one_hot_embeddings)\n",
    "        previous_char_slice = Lambda(lambda x: x[:, :-1,:], output_shape=(self.sample_out_size, nclasses))(padding)\n",
    "        \n",
    "        #Step 2: Define the recurrent layer. \n",
    "        lstm = SC_LSTM(\n",
    "            lstm_units,\n",
    "            nclasses,\n",
    "            softmax_temperature=None,\n",
    "            return_da=True,\n",
    "            return_state=False,\n",
    "            use_bias=True,\n",
    "            return_sequences=True,\n",
    "            implementation=2,\n",
    "            dropout=0.2,\n",
    "            recurrent_dropout=0.2,\n",
    "            sc_dropout=0.2\n",
    "        )\n",
    "        \n",
    "        generated_output, da_t, da_history = lstm([previous_char_slice, meaning_representation])\n",
    "        \n",
    "        # == == == == ==#\n",
    "        # Define Losses #\n",
    "        # == == == == ==#\n",
    "        \n",
    "        #Reconstruction Loss\n",
    "        def vae_cross_ent_loss(args):\n",
    "            x_truth, x_decoded_final = args\n",
    "            x_truth_flatten = K.reshape(x_truth, shape=(-1, K.shape(x_truth)[-1]))\n",
    "            x_decoded_flat = K.reshape(x_decoded_final, shape=(-1, K.shape(x_decoded_final)[-1]))\n",
    "            cross_ent = K.categorical_crossentropy(x_truth_flatten, x_decoded_flat)\n",
    "            cross_ent = K.reshape(cross_ent, shape=(-1, K.shape(x_truth)[1]))\n",
    "            sum_over_sentences = K.sum(cross_ent, axis=1)\n",
    "            return sum_over_sentences\n",
    "        \n",
    "        #Make sure the MR vector converges to 0, i.e. all the attributes have been consumend\n",
    "        def da_loss_fun(args):\n",
    "            da = args[0]\n",
    "            return K.l2_normalize(da, axis=1)\n",
    "        \n",
    "        #Make sure the changes in the MR vector are not too large\n",
    "        def da_history_loss_fun(args):\n",
    "            da_t = args[0]\n",
    "            zeta = 100\n",
    "            n = 1e-4\n",
    "            # shape: batch_size, sample_size\n",
    "            norm_of_differnece = K.sum(n*zeta**K.l2_normalize(da_t[:, 1:, :] - da_t[:, :-1, :], axis=1), axis=2)\n",
    "            n1 =norm_of_differnece\n",
    "            return K.sum(n1, axis=1)\n",
    "        \n",
    "        main_loss = Lambda(vae_cross_ent_loss, output_shape=(1,), name='main')([output_one_hot_embeddings, generated_output])\n",
    "        da_loss = Lambda(da_loss_fun, output_shape=(1,), name='dialogue_act')([da_t])\n",
    "        da_history_loss = Lambda(da_history_loss_fun, output_shape=(1,), name='dialogue_history')([da_history])\n",
    "        \n",
    "        self.train_model = Model(inputs=inputs + [output_idx], outputs=[main_loss, da_loss, da_history_loss])\n",
    "        \n",
    "        # == == == == == #\n",
    "        # Define Outputs #\n",
    "        # == == == == == #\n",
    "        \n",
    "        argmax = Lambda(lambda x: K.argmax(x, axis=2), output_shape=(self.sample_out_size,))(generated_output)\n",
    "        self.test_model = Model(inputs=inputs + [output_idx], outputs=argmax)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start the training procedure, we need to make sure, that we can generate some outputs during the training phase. This is important because the losses alone are not enough to get a good grasp of the performance. Thus, we exploit some nice properties of Keras: custom callbacks. \n",
    "Callbacks are funcions, which can be passed to the training procedure. You can decide when a particular function should be executed: begin or end of epoch, begin or end of batch, begin or end of training. We want to output the predictions for the test-set at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "\n",
    "class OutputCallback(Callback):\n",
    "    def __init__(self, test_model, model_input, da_acts, lex_dict, delex_vocab, char_vocab, delimiter='', fname='../../logging/test_output'):\n",
    "        self.model_input = model_input\n",
    "        self.char_vocab = char_vocab\n",
    "        self.test_model = test_model\n",
    "        self.delimiter = delimiter\n",
    "        self.fname = fname\n",
    "        self.lex_dict = lex_dict\n",
    "        self.da_acts = da_acts\n",
    "        self.delex_vocab = delex_vocab\n",
    "\n",
    "        super(OutputCallback, self).__init__()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        ofile = open('{}_{}.txt'.format(self.fname, str(epoch)), 'wt', encoding='utf-8')\n",
    "        inverse_vocab = {v: k for (k, v) in self.char_vocab.items()}\n",
    "        \n",
    "        predictions = self.test_model.predict(self.model_input, batch_size=1024, verbose=1)\n",
    "        \n",
    "        sen_dict = []\n",
    "        for i, sentence in enumerate(predictions):\n",
    "            list_txt_idx = [int(x) for x in sentence.tolist()]\n",
    "            txt_list = [inverse_vocab.get(int(x), '') for x in list_txt_idx]\n",
    "            oline = self.delimiter.join(txt_list)\n",
    "            for lex_key, val in self.lex_dict.items():\n",
    "                original_token = self.delex_vocab[lex_key][i]\n",
    "                oline = oline.replace(val, original_token)\n",
    "            sen_dict.append(oline)\n",
    "        \n",
    "            \n",
    "        for sentence in sen_dict:\n",
    "            ofile.write('{}\\n'.format(sentence))\n",
    "        ofile.write('\\n')\n",
    "        ofile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to prepare the input to the neural network. For this, we use a helper function, which creates an array of inputs. These input match the symbolic input layers, which we defined in the neural network architecture. The output of the neural network are the three losses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The delexicalized fileds:\n",
      "{'food': 'XFOODX',\n",
      " 'name': 'XNAMEX',\n",
      " 'near': 'XNESRX'}\n",
      "Example of Delex Vocabulary: \n",
      "Name: The Cricketers\n",
      "Near: Avalon\n",
      "Food: \n"
     ]
    }
   ],
   "source": [
    "from src.data_processing.delexicalise_data import _get_delex_fields\n",
    "from collections import defaultdict\n",
    "model_object = SCLSTM(False, char_vocab, max_sentence_len)\n",
    "\n",
    "def _prepare_input(mr_vecs, idx_data, surface_features):\n",
    "    sem_input = sorted(list(mr_vecs.items()), key=lambda x: x[0])\n",
    "    sem_input = [x[1] for x in sem_input]\n",
    "    \n",
    "    if surface_features is not None:\n",
    "        input_data = sem_input + [surface_features]\n",
    "    else:\n",
    "        input_data = sem_input\n",
    "    \n",
    "    input_data += [idx_data]\n",
    "    output_data = [np.ones(len(input_data[0]))] * 3\n",
    "    \n",
    "    return input_data, output_data\n",
    "\n",
    "def _get_lexicalize_dict(parsed_mrs, delex_fields):\n",
    "    \"\"\"\n",
    "    Helper function, which creates a mapping for each delexicalized attribute to the correct value. \n",
    "    This is done to replace the placeholders by the correct value at the end.\n",
    "    \"\"\"\n",
    "    delex_vocabulary = defaultdict(lambda: [])\n",
    "    for attribute, replacement_token in delex_fields.items():\n",
    "        values = [x.get(attribute, '') for x in parsed_mrs]\n",
    "        delex_vocabulary[attribute] = values\n",
    "    return delex_vocabulary\n",
    "\n",
    "train_input, train_output =  _prepare_input(train_mr_vecs, train_idx_data, None)\n",
    "valid_input, valid_output =  _prepare_input(valid_mr_vecs, valid_idx_data, None)\n",
    "test_input, _ = _prepare_input(test_vectorised_mrs_only, test_mr_only_dummy_idx, None)\n",
    "\n",
    "delex_fields = _get_delex_fields(attribute_tokens, delex_attributes)\n",
    "print('The delexicalized fileds:')\n",
    "pp.pprint(delex_fields)\n",
    "\n",
    "test_delex_vocab = _get_lexicalize_dict(test_process_mr_only, delex_fields)\n",
    "print('Example of Delex Vocabulary: ')\n",
    "print('Name:', test_delex_vocab['name'][idx])\n",
    "print('Near:', test_delex_vocab['near'][idx])\n",
    "print('Food:', test_delex_vocab['food'][idx])\n",
    "\n",
    "\n",
    "output_callback = OutputCallback(\n",
    "    model_object.test_model, \n",
    "    test_input, \n",
    "    test_process_mr_only, \n",
    "    delex_fields, \n",
    "    test_delex_vocab, \n",
    "    char_vocab\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42061 samples, validate on 4672 samples\n",
      "Epoch 1/100\n",
      "42061/42061 [==============================] - 628s 15ms/step - loss: 532.0195 - main_loss: 531.2571 - dialogue_act_loss: 0.0000e+00 - dialogue_history_loss: 0.7624 - val_loss: 874.8748 - val_main_loss: 874.1125 - val_dialogue_act_loss: 0.0000e+00 - val_dialogue_history_loss: 0.7623\n",
      "630/630 [==============================] - 3s 6ms/step\n",
      "Epoch 2/100\n",
      " 4096/42061 [=>............................] - ETA: 9:06 - loss: 431.1542 - main_loss: 430.3919 - dialogue_act_loss: 0.0000e+00 - dialogue_history_loss: 0.7623"
     ]
    }
   ],
   "source": [
    "model_object.train_model.compile(optimizer=model_object.optimizer, loss=lambda y_true, y_pred: y_pred)\n",
    "model_object.train_model.fit(\n",
    "    x=train_input,\n",
    "    y=train_output,\n",
    "    epochs=100,\n",
    "    batch_size=256,\n",
    "    validation_data=(valid_input, valid_output),\n",
    "    callbacks=[output_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras_std]",
   "language": "python",
   "name": "conda-env-keras_std-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
