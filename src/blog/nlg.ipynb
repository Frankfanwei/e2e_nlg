{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hands On\n",
    "\n",
    "Now let's get our hands dirty. We show how the whole training pipeline works and then we also look at how to write a custom Keras Layer for the SC-LSTM cell. You can donwload the whole codebase here: [Cool Code Here](https://github.com/jderiu/e2e_nlg) \n",
    "\n",
    "## Preprocessing\n",
    "The first step in every machine learning pipeline is the preprocessing. The preprocessing consists of the following steps:\n",
    "- Delexicalizing the data: Replacing the names of the restaurant by placeholders. \n",
    "- Vectorizing the data: translating the meaning represenations into binarized vectors as well as transforming the utterances in a list of indices, each character is represented by an index from the vocabulary (i.e. char2idx mapping).\n",
    "- Extracting the syntactic information: get the first word of the utterance and the follow-up sentences and encode those into a binary vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-11 14:47:18,890 : INFO : Base directory:C:\\Users\\deri\\Documents\\Git Projects\\e2e_nlg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of Trainset: 42061 Validationset: 4672 Testset: 4693\n",
      "Parsed MR:\n",
      "{'area': 'riverside',\n",
      " 'customer rating': '5 out of 5',\n",
      " 'eatType': 'coffee shop',\n",
      " 'food': 'Japanese',\n",
      " 'name': 'The Golden Palace',\n",
      " 'priceRange': 'more than £30'}\n",
      "Original Output:  The coffee shop The Golden Palace is north of the city centre. It serves expensive food and has a 5 star rating.\n",
      "Delexicalized Output:  The coffee shop XNAMEX is north of the city centre. It serves expensive food and has a 5 star rating.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os, sys\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(width=41, compact=True)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#makes sure that the modules can be loaded\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "nb_dir = nb_dir.replace('\\\\src', '')\n",
    "sys.path.append(nb_dir)\n",
    "logging.info('Base directory:' + nb_dir)\n",
    "## should output: /some-path/e2e_nlg\n",
    "\n",
    "from src.data_processing.delexicalise_data import _delex_nlg_data, _retrieve_mr_ontology\n",
    "data_path = os.path.join(nb_dir, 'data/e2e_nlg')\n",
    "# List of what attributes we want to replace with a placeholder\n",
    "delex_attributes = [\"name\", \"near\", \"food\"]\n",
    "# File name for the table of attribute-placeholder pairs\n",
    "attribute_fname = 'ontology/attribute_tags.txt'\n",
    "\n",
    "train_delex = _delex_nlg_data('trainset.csv', data_path, delex_attributes, attribute_fname)\n",
    "valid_delex = _delex_nlg_data('devset.csv', data_path, delex_attributes, attribute_fname)\n",
    "test_delex = _delex_nlg_data('testset.csv', data_path, delex_attributes, attribute_fname)\n",
    "\n",
    "print('Lengths of Trainset: {} Validationset: {} Testset: {}'.format(\n",
    "    len(train_delex['mr_raw']), \n",
    "    len(valid_delex['mr_raw']), \n",
    "    len(test_delex['mr_raw'])))\n",
    "\n",
    "#Print an example\n",
    "idx = 110 #(change me)\n",
    "print('Parsed MR:')\n",
    "pp.pprint(train_delex['parsed_mrs'][idx])\n",
    "print('Original Output: ', train_delex['outputs_raw'][idx])\n",
    "print('Delexicalized Output: ', train_delex['delexicalised_texts'][idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to extract the data ontology, which we need to vectorize the data later. The ontoloty is a dictonary of values2idx vocabulaires. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of attributes:\n",
      "['name', 'eatType', 'priceRange',\n",
      " 'customer rating', 'near', 'food',\n",
      " 'area', 'familyFriendly']\n",
      "Value2Idx Vocabulary for priceRange:\n",
      "{'cheap': 0,\n",
      " 'high': 1,\n",
      " 'less than £20': 2,\n",
      " 'moderate': 3,\n",
      " 'more than £30': 4,\n",
      " '£20-25': 5}\n"
     ]
    }
   ],
   "source": [
    "full_mr_list = train_delex['parsed_mrs'] + valid_delex['parsed_mrs'] + test_delex['parsed_mrs']\n",
    "mr_data_ontology = _retrieve_mr_ontology(full_mr_list)\n",
    "print('List of attributes:')\n",
    "pp.pprint(list(mr_data_ontology.keys()))\n",
    "print('Value2Idx Vocabulary for priceRange:')\n",
    "pp.pprint(mr_data_ontology['priceRange'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "Next, we transform the preprocessed data into vectors, which can be interpreted by our neural network. This requires the following steps:\n",
    "- Transform the meaning representations into a binary representation. For this, we rely on the ontology we extracted in the cell above.\n",
    "- Transform the utterances into a list of indices, which are then given as input to the neural network. Each index corresponds to a alphanumeric character.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Meaning Representations\n",
    "For each attribute, we create a one-hot encoded vector, which indicates which value is present in the utterance. We add am extra dimension to the vectors for those cases where the attrbute is missing. Note that the delexicalized attributes only have lenghts of two. This is just to indicate if the attribute is present or not, since the value is replaced by a placeholder.\n",
    "\n",
    "We frist take the processing of one MR apart and then we process the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'area': 3,\n",
      " 'customer rating': 7,\n",
      " 'eatType': 4,\n",
      " 'familyFriendly': 3,\n",
      " 'food': 2,\n",
      " 'name': 2,\n",
      " 'near': 2,\n",
      " 'priceRange': 7}\n"
     ]
    }
   ],
   "source": [
    "from src.data_processing.vectorize_data import _compute_vector_length, _vectorize_single_mr\n",
    "\n",
    "# First compute the length of the one-hot encoded vectors:\n",
    "vector_lengts = _compute_vector_length(mr_data_ontology, delex_attributes)\n",
    "pp.pprint(vector_lengts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'more than £30'\n",
      "array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [1.],\n",
      "       [0.],\n",
      "       [0.]])\n"
     ]
    }
   ],
   "source": [
    "#Process one meaning representation:\n",
    "mr = train_delex['parsed_mrs'][idx]\n",
    "vec = _vectorize_single_mr(mr, mr_data_ontology, vector_lengts, delex_attributes)\n",
    "pp.pprint(train_delex['parsed_mrs'][idx]['priceRange'])\n",
    "pp.pprint(vec['priceRange'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the meaning representation vectorization over the whole dataset. We store the result in a dictionary of attribute name to vectors. Each row corresponds to one datapoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: (42061, 7)\n"
     ]
    }
   ],
   "source": [
    "# Vectorize meaning representations\n",
    "from src.data_processing.vectorize_data import _vectorize_mrs\n",
    "\n",
    "train_mr_vecs = _vectorize_mrs(train_delex['parsed_mrs'], mr_data_ontology, delex_attributes)\n",
    "valid_mr_vecs = _vectorize_mrs(valid_delex['parsed_mrs'], mr_data_ontology, delex_attributes)\n",
    "test_mr_vecs = _vectorize_mrs(test_delex['parsed_mrs'], mr_data_ontology, delex_attributes)\n",
    "print('Dimensions: {}'.format(train_mr_vecs['priceRange'].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Utterances\n",
    "Now we just need to create the representation for the utterances. For this load the vocabulary and then just apply the transforamtion. One important detail: since Keras works with fixed lenght sequences, we need to pad the texts (or cut them off) so that all the vectors have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Len: 68\n",
      "Idx of character \"a\": 5\n",
      "Dummy Idx: 68 Unknown Idx: 69\n",
      "Input Data Shape: (42061, 256)\n"
     ]
    }
   ],
   "source": [
    "#Step 1 Load Vocabulary\n",
    "import json\n",
    "from src.data_processing.utils import convert2indices \n",
    "\n",
    "char_fname = open(os.path.join(data_path, 'vocabulary.json'), 'rt', encoding='utf-8')\n",
    "char_vocab = json.load(char_fname)\n",
    "print('Vocab Len: {}'.format(len(char_vocab)))\n",
    "print('Idx of character \"a\": {}'.format(char_vocab['a']))\n",
    "\n",
    "#Always use a dummy character for padding and a unk character for unknown tokens (or characters in this case)\n",
    "dummy_char = max(char_vocab.values()) + 1\n",
    "unk_char = max(char_vocab.values()) + 2\n",
    "\n",
    "print('Dummy Idx: {} Unknown Idx: {}'.format(dummy_char, unk_char))\n",
    "\n",
    "#Step 2 Convert 2 Indices\n",
    "max_sentence_len = 256\n",
    "\n",
    "train_idx_data = convert2indices(train_delex['delexicalised_texts'], char_vocab, dummy_char, unk_char, max_sentence_len)\n",
    "valid_idx_data = convert2indices(valid_delex['delexicalised_texts'], char_vocab, dummy_char, unk_char, max_sentence_len)\n",
    "test_idx_data = convert2indices(test_delex['delexicalised_texts'], char_vocab, dummy_char, unk_char, max_sentence_len)\n",
    "\n",
    "#The shape is number of datapoints x sentence length\n",
    "print('Input Data Shape: {}'.format(train_idx_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Word Features\n",
    "Next we prepare the syntactic features, which we use for to add more variety to the generated utterances. For sake of brievety, we only show the extraction of the first word features. The other two manipulations are present in the full code version on Github. \n",
    "\n",
    "The extraciton of the first word is done in following steps:\n",
    "- Word tokenize all delexicalized utterances.\n",
    "- Extract the first word of each utterance. \n",
    "- Create a vocabulary of first words, i.e. first word-to-idx mapping. We only keep first words, which appear at least 100 times. Otherwise the neural network has difficulties learning the correlation between the first word and the utterance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'coffee', 'shop', 'XNAMEX', 'is', 'north', 'of', 'the', 'city', 'centre', '.'], ['It', 'serves', 'expensive', 'food', 'and', 'has', 'a', '5', 'star', 'rating', '.']]\n",
      "Mapping from First Word 2 Index:\n",
      "[('XNAMEX', 0), ('Located', 1),\n",
      " ('For', 2), ('In', 3), ('A', 4),\n",
      " ('XNESRX', 5), ('An', 6), ('Near', 7),\n",
      " ('There', 8), ('On', 9), ('XFOODX', 10),\n",
      " ('The', 11), ('With', 12),\n",
      " ('Serving', 13), ('If', 14), ('At', 15),\n",
      " ('Riverside', 16), ('By', 17),\n",
      " ('You', 18), ('Family', 19)]\n",
      "Shape of First words: (42061, 21)\n",
      "The word \"The\" corresponds to : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Tokenize the Utterances\n",
    "from src.data_processing.surface_feature_vectors import _sentence_tok, _utterance_first_word_vocab, _utt_fw_features\n",
    "\n",
    "train_tok = _sentence_tok(train_delex['delexicalised_texts'])\n",
    "valid_tok = _sentence_tok(valid_delex['delexicalised_texts'])\n",
    "test_tok = _sentence_tok(test_delex['delexicalised_texts'])\n",
    "\n",
    "#Print an example: Note that we both tokenize on sentence and word level. The sentence level tokeniztion can be used for other manipulations.\n",
    "print(train_tok[idx])\n",
    "\n",
    "#Step 2: Generate fw2idx mapping\n",
    "utt_fw_vocab = _utterance_first_word_vocab(train_tok + valid_tok + test_tok, min_freq=100)\n",
    "inverse_utt_fw_vocab = {v: k for k, v in utt_fw_vocab.items()}\n",
    "print('Mapping from First Word 2 Index:')\n",
    "pp.pprint(list(utt_fw_vocab.items()))\n",
    "\n",
    "#Step 3 Create Surface Level Features\n",
    "train_utt_fw = _utt_fw_features(train_tok, utt_fw_vocab)\n",
    "valid_utt_fw = _utt_fw_features(valid_tok, utt_fw_vocab)\n",
    "test_utt_fw = _utt_fw_features(test_tok, utt_fw_vocab)\n",
    "\n",
    "print('Shape of First words: {}'.format(train_utt_fw.shape))\n",
    "print('The word \"{}\" corresponds to : {}'.format(train_tok[idx][0][0], train_utt_fw[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WAIT A MOMENT: THAT'S CHEATING !!!!\n",
    "Of course, we do nat have access to the correct first word during test time. This is indeed a major drawback of this approach. The solution is to sample n different first words for each meaning representation during test time. This then corresponds to n different utterances. Then we have to rank those utterances according to their semantic correctness, as there are conficlting combinations of meaning represenations and first words. For instance, when there is no location mentioned but the first word is \"Located\". \n",
    "\n",
    "So let's sample 10 different first words for each meaning representation in the test set. We have an extra test set which contains only the meaning representations (i.e. no reference utterances given)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of Test Set: 630\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "import numpy as np\n",
    "import random\n",
    "from src.data_processing.generate_evaluation_data import _read_data, _parse_raw_mr\n",
    "test_mr_only = os.path.join(data_path, 'test_mr_only.csv')\n",
    "\n",
    "#Read the MR only test set \n",
    "test_mr_only_raw = _read_data(test_mr_only)\n",
    "test_process_mr_only = _parse_raw_mr(test_mr_only_raw)\n",
    "test_vectorised_mrs_only = _vectorize_mrs(test_process_mr_only, mr_data_ontology, delex_attributes)\n",
    "\n",
    "def sample_utt_fw_for_mr(nsamples):\n",
    "    utt_fw_samples = random.sample(list(utt_fw_vocab.values()), k=nsamples)\n",
    "    dummy_idx = max(utt_fw_vocab.values()) + 1\n",
    "    utt_fw_vec = []\n",
    "    for fidx in utt_fw_samples:\n",
    "        v = np.zeros(shape=(dummy_idx + 1, 1))\n",
    "        v[fidx] = 1.0\n",
    "        utt_fw_vec.append(v)\n",
    "    return utt_fw_vec\n",
    "    \n",
    "first_word_features = []    \n",
    "for mr in test_process_mr_only:\n",
    "    utt_fw_vec = sample_utt_fw_for_mr(10)\n",
    "    first_word_features.append(utt_fw_vec)\n",
    "\n",
    "print('Len of Test Set: {}'.format(len(first_word_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras_std]",
   "language": "python",
   "name": "conda-env-keras_std-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
