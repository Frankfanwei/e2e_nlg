{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hands On\n",
    "\n",
    "Now let's get our hands dirty. We show how the whole training pipeline works and then we also look at how to write a custom Keras Layer for the SC-LSTM cell. You can donwload the whole codebase here: [Cool Code Here](https://github.com/jderiu/e2e_nlg) \n",
    "\n",
    "## Preprocessing\n",
    "The first step in every machine learning pipeline is the preprocessing. The preprocessing consists of the following steps:\n",
    "- Delexicalizing the data: Replacing the names of the restaurant by placeholders. \n",
    "- Vectorizing the data: translating the meaning represenations into binarized vectors as well as transforming the utterances in a list of indices, each character is represented by an index from the vocabulary (i.e. char2idx mapping).\n",
    "- Extracting the syntactic information: get the first word of the utterance and the follow-up sentences and encode those into a binary vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-13 21:38:45,166 : INFO : Base directory:D:\\GitRepos\\e2e_nlg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of Trainset: 42061 Validationset: 4672 Testset: 4693\n",
      "Parsed MR:\n",
      "{'area': 'riverside',\n",
      " 'customer rating': '5 out of 5',\n",
      " 'eatType': 'coffee shop',\n",
      " 'food': 'Japanese',\n",
      " 'name': 'The Golden Palace',\n",
      " 'priceRange': 'more than £30'}\n",
      "Original Output:  The coffee shop The Golden Palace is north of the city centre. It serves expensive food and has a 5 star rating.\n",
      "Delexicalized Output:  The coffee shop XNAMEX is north of the city centre. It serves expensive food and has a 5 star rating.\n",
      "{'area': 'XAREAX',\n",
      " 'customer rating': 'XCUSTX',\n",
      " 'eatType': 'XEATX',\n",
      " 'familyFriendly': 'XFAMX',\n",
      " 'food': 'XFOODX',\n",
      " 'name': 'XNAMEX',\n",
      " 'near': 'XNESRX',\n",
      " 'priceRange': 'XPRICX'}\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os, sys\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(width=41, compact=True)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#makes sure that the modules can be loaded\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "nb_dir = nb_dir.replace('\\\\src', '')\n",
    "sys.path.append(nb_dir)\n",
    "logging.info('Base directory:' + nb_dir)\n",
    "## should output: /some-path/e2e_nlg\n",
    "\n",
    "from src.data_processing.delexicalise_data import _delex_nlg_data, _retrieve_mr_ontology, _load_attributes\n",
    "data_path = os.path.join(nb_dir, 'data/e2e_nlg')\n",
    "# List of what attributes we want to replace with a placeholder\n",
    "delex_attributes = [\"name\", \"near\", \"food\"]\n",
    "# File name for the table of attribute-placeholder pairs\n",
    "attribute_fname = 'ontology/attribute_tags.txt'\n",
    "attribute_tokens = _load_attributes(os.path.join(data_path, attribute_fname))\n",
    "\n",
    "train_delex = _delex_nlg_data('trainset.csv', data_path, delex_attributes, attribute_tokens)\n",
    "valid_delex = _delex_nlg_data('devset.csv', data_path, delex_attributes, attribute_tokens)\n",
    "test_delex = _delex_nlg_data('testset.csv', data_path, delex_attributes, attribute_tokens)\n",
    "\n",
    "print('Lengths of Trainset: {} Validationset: {} Testset: {}'.format(\n",
    "    len(train_delex['mr_raw']), \n",
    "    len(valid_delex['mr_raw']), \n",
    "    len(test_delex['mr_raw'])))\n",
    "\n",
    "#Print an example\n",
    "idx = 110 #(change me)\n",
    "print('Parsed MR:')\n",
    "pp.pprint(train_delex['parsed_mrs'][idx])\n",
    "print('Original Output: ', train_delex['outputs_raw'][idx])\n",
    "print('Delexicalized Output: ', train_delex['delexicalised_texts'][idx])\n",
    "pp.pprint(attribute_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to extract the data ontology, which we need to vectorize the data later. The ontoloty is a dictonary of values2idx vocabulaires. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of attributes:\n",
      "['name', 'eatType', 'priceRange',\n",
      " 'customer rating', 'near', 'food',\n",
      " 'area', 'familyFriendly']\n",
      "Value2Idx Vocabulary for priceRange:\n",
      "{'cheap': 0,\n",
      " 'high': 1,\n",
      " 'less than £20': 2,\n",
      " 'moderate': 3,\n",
      " 'more than £30': 4,\n",
      " '£20-25': 5}\n"
     ]
    }
   ],
   "source": [
    "full_mr_list = train_delex['parsed_mrs'] + valid_delex['parsed_mrs'] + test_delex['parsed_mrs']\n",
    "mr_data_ontology = _retrieve_mr_ontology(full_mr_list)\n",
    "print('List of attributes:')\n",
    "pp.pprint(list(mr_data_ontology.keys()))\n",
    "print('Value2Idx Vocabulary for priceRange:')\n",
    "pp.pprint(mr_data_ontology['priceRange'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "Next, we transform the preprocessed data into vectors, which can be interpreted by our neural network. This requires the following steps:\n",
    "- Transform the meaning representations into a binary representation. For this, we rely on the ontology we extracted in the cell above.\n",
    "- Transform the utterances into a list of indices, which are then given as input to the neural network. Each index corresponds to a alphanumeric character.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Meaning Representations\n",
    "For each attribute, we create a one-hot encoded vector, which indicates which value is present in the utterance. We add am extra dimension to the vectors for those cases where the attrbute is missing. Note that the delexicalized attributes only have lenghts of two. This is just to indicate if the attribute is present or not, since the value is replaced by a placeholder.\n",
    "\n",
    "We frist take the processing of one MR apart and then we process the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'area': 3,\n",
      " 'customer rating': 7,\n",
      " 'eatType': 4,\n",
      " 'familyFriendly': 3,\n",
      " 'food': 2,\n",
      " 'name': 2,\n",
      " 'near': 2,\n",
      " 'priceRange': 7}\n"
     ]
    }
   ],
   "source": [
    "from src.data_processing.vectorize_data import _compute_vector_length, _vectorize_single_mr\n",
    "\n",
    "# First compute the length of the one-hot encoded vectors:\n",
    "vector_lengts = _compute_vector_length(mr_data_ontology, delex_attributes)\n",
    "pp.pprint(vector_lengts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'more than £30'\n",
      "array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [1.],\n",
      "       [0.],\n",
      "       [0.]])\n"
     ]
    }
   ],
   "source": [
    "#Process one meaning representation:\n",
    "mr = train_delex['parsed_mrs'][idx]\n",
    "vec = _vectorize_single_mr(mr, mr_data_ontology, vector_lengts, delex_attributes)\n",
    "pp.pprint(train_delex['parsed_mrs'][idx]['priceRange'])\n",
    "pp.pprint(vec['priceRange'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the meaning representation vectorization over the whole dataset. We store the result in a dictionary of attribute name to vectors. Each row corresponds to one datapoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: (42061, 7)\n"
     ]
    }
   ],
   "source": [
    "# Vectorize meaning representations\n",
    "from src.data_processing.vectorize_data import _vectorize_mrs\n",
    "\n",
    "train_mr_vecs = _vectorize_mrs(train_delex['parsed_mrs'], mr_data_ontology, delex_attributes)\n",
    "valid_mr_vecs = _vectorize_mrs(valid_delex['parsed_mrs'], mr_data_ontology, delex_attributes)\n",
    "test_mr_vecs = _vectorize_mrs(test_delex['parsed_mrs'], mr_data_ontology, delex_attributes)\n",
    "print('Dimensions: {}'.format(train_mr_vecs['priceRange'].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Utterances\n",
    "Now we just need to create the representation for the utterances. For this load the vocabulary and then just apply the transforamtion. One important detail: since Keras works with fixed lenght sequences, we need to pad the texts (or cut them off) so that all the vectors have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Len: 68\n",
      "Idx of character \"a\": 5\n",
      "Dummy Idx: 68 Unknown Idx: 69\n",
      "Input Data Shape: (42061, 256)\n"
     ]
    }
   ],
   "source": [
    "#Step 1 Load Vocabulary\n",
    "import json\n",
    "from src.data_processing.utils import convert2indices \n",
    "\n",
    "char_fname = open(os.path.join(data_path, 'vocabulary.json'), 'rt', encoding='utf-8')\n",
    "char_vocab = json.load(char_fname)\n",
    "print('Vocab Len: {}'.format(len(char_vocab)))\n",
    "print('Idx of character \"a\": {}'.format(char_vocab['a']))\n",
    "\n",
    "#Always use a dummy character for padding and a unk character for unknown tokens (or characters in this case)\n",
    "dummy_char = max(char_vocab.values()) + 1\n",
    "unk_char = max(char_vocab.values()) + 2\n",
    "\n",
    "print('Dummy Idx: {} Unknown Idx: {}'.format(dummy_char, unk_char))\n",
    "\n",
    "#Step 2 Convert to Indices\n",
    "max_sentence_len = 256\n",
    "\n",
    "train_idx_data = convert2indices(train_delex['delexicalised_texts'], char_vocab, dummy_char, unk_char, max_sentence_len)\n",
    "valid_idx_data = convert2indices(valid_delex['delexicalised_texts'], char_vocab, dummy_char, unk_char, max_sentence_len)\n",
    "test_idx_data = convert2indices(test_delex['delexicalised_texts'], char_vocab, dummy_char, unk_char, max_sentence_len)\n",
    "\n",
    "#The shape is number of datapoints x sentence length\n",
    "print('Input Data Shape: {}'.format(train_idx_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Word Features\n",
    "Next we prepare the syntactic features, which we use for to add more variety to the generated utterances. For sake of brievety, we only show the extraction of the first word features. The other two manipulations are present in the full code version on Github. \n",
    "\n",
    "The extraciton of the first word is done in following steps:\n",
    "- Word tokenize all delexicalized utterances.\n",
    "- Extract the first word of each utterance. \n",
    "- Create a vocabulary of first words, i.e. first word-to-idx mapping. We only keep first words, which appear at least 100 times. Otherwise the neural network has difficulties learning the correlation between the first word and the utterance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'coffee', 'shop', 'XNAMEX', 'is', 'north', 'of', 'the', 'city', 'centre', '.'], ['It', 'serves', 'expensive', 'food', 'and', 'has', 'a', '5', 'star', 'rating', '.']]\n",
      "Mapping from First Word 2 Index:\n",
      "[('XNAMEX', 0), ('Located', 1),\n",
      " ('For', 2), ('In', 3), ('A', 4),\n",
      " ('XNESRX', 5), ('An', 6), ('Near', 7),\n",
      " ('There', 8), ('On', 9), ('XFOODX', 10),\n",
      " ('The', 11), ('With', 12),\n",
      " ('Serving', 13), ('If', 14), ('At', 15),\n",
      " ('Riverside', 16), ('By', 17),\n",
      " ('You', 18), ('Family', 19)]\n",
      "Shape of First words: (42061, 21)\n",
      "The word \"The\" corresponds to : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Tokenize the Utterances\n",
    "from src.data_processing.surface_feature_vectors import _sentence_tok, _utterance_first_word_vocab, _utt_fw_features\n",
    "\n",
    "train_tok = _sentence_tok(train_delex['delexicalised_texts'])\n",
    "valid_tok = _sentence_tok(valid_delex['delexicalised_texts'])\n",
    "test_tok = _sentence_tok(test_delex['delexicalised_texts'])\n",
    "\n",
    "#Print an example: Note that we both tokenize on sentence and word level. The sentence level tokeniztion can be used for other manipulations.\n",
    "print(train_tok[idx])\n",
    "\n",
    "#Step 2: Generate fw2idx mapping\n",
    "utt_fw_vocab = _utterance_first_word_vocab(train_tok + valid_tok + test_tok, min_freq=100)\n",
    "inverse_utt_fw_vocab = {v: k for k, v in utt_fw_vocab.items()}\n",
    "print('Mapping from First Word 2 Index:')\n",
    "pp.pprint(list(utt_fw_vocab.items()))\n",
    "\n",
    "#Step 3 Create Surface Level Features\n",
    "train_utt_fw = _utt_fw_features(train_tok, utt_fw_vocab)\n",
    "valid_utt_fw = _utt_fw_features(valid_tok, utt_fw_vocab)\n",
    "test_utt_fw = _utt_fw_features(test_tok, utt_fw_vocab)\n",
    "\n",
    "utt_fw_input_dimension = train_utt_fw.shape[1]\n",
    "print('Shape of First words: {}'.format(train_utt_fw.shape))\n",
    "print('The word \"{}\" corresponds to : {}'.format(train_tok[idx][0][0], train_utt_fw[idx]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WAIT A MOMENT: THAT'S CHEATING !!!!\n",
    "Of course, we do nat have access to the correct first word during test time. This is indeed a major drawback of this approach. The solution is to sample n different first words for each meaning representation during test time. This then corresponds to n different utterances. Then we have to rank those utterances according to their semantic correctness, as there are conficlting combinations of meaning represenations and first words. For instance, when there is no location mentioned but the first word is \"Located\". \n",
    "\n",
    "So let's sample 10 different first words for each meaning representation in the test set. We have an extra test set which contains only the meaning representations (i.e. no reference utterances given)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of Test Set: 630\n",
      "(630, 2)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "import numpy as np\n",
    "import random\n",
    "from src.data_processing.generate_evaluation_data import _read_data, _parse_raw_mr\n",
    "test_mr_only = os.path.join(data_path, 'test_mr_only.csv')\n",
    "\n",
    "#Read the MR only test set \n",
    "test_mr_only_raw = _read_data(test_mr_only)\n",
    "test_process_mr_only = _parse_raw_mr(test_mr_only_raw)\n",
    "test_vectorised_mrs_only = _vectorize_mrs(test_process_mr_only, mr_data_ontology, delex_attributes)\n",
    "test_mr_only_dummy_idx = np.zeros(shape=(len(test_mr_only_raw), max_sentence_len)) #dummy output idx\n",
    "\n",
    "def sample_utt_fw_for_mr(nsamples):\n",
    "    utt_fw_samples = random.sample(list(utt_fw_vocab.values()), k=nsamples)\n",
    "    dummy_idx = max(utt_fw_vocab.values()) + 1\n",
    "    utt_fw_vec = []\n",
    "    for fidx in utt_fw_samples:\n",
    "        v = np.zeros(shape=(dummy_idx + 1, ))\n",
    "        v[fidx] = 1.0\n",
    "        utt_fw_vec.append(v)\n",
    "    return utt_fw_vec\n",
    "    \n",
    "first_word_features = []    \n",
    "for mr in test_process_mr_only:\n",
    "    utt_fw_vec = sample_utt_fw_for_mr(10)\n",
    "    first_word_features.append(utt_fw_vec)\n",
    "\n",
    "print('Len of Test Set: {}'.format(len(first_word_features)))\n",
    "print(test_vectorised_mrs_only['name'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally some Deep Learning\n",
    "\n",
    "The preprocessing is finally done, now we can focus on training the semantically conditioned LSTM. For this we build the architecture and then start the training procedure. \n",
    "\n",
    "The architecture is rather simple: \n",
    "- Inputs\n",
    "    - We define inputs for each of the eight possible attribues form the domain ontoloty. \n",
    "    - We define inputs for the syntactic features (in this case, we only look at the first word of the utterance).\n",
    "    - We define the input for the expected output utterance, which is used to perform teacher forcing and to comput the reconstriction loss.\n",
    "- Embeddings. Since we work on the character level, we just use a one-hot representation of the characters. This menans that we represent each character by a vector, where the character index is set to 1.\n",
    "- Generator. We use a custom made SC-LSTM Layer (more on this in the next post), which has two inputs and three outputs:\n",
    "    - Input 1: The meaning representation + syntactic representation input.\n",
    "    - Input 2: The previously generated token. This is important as the generator needs to learn to produce the next character given the current cell state and the previously generated token. For this we just shift the output-utterance by one to the right.\n",
    "    - Outputs: The generated utterance, the last state of the extra cell (meaning representation cell) and the history of all meaning representation states. These outputs are used to compute the loss. $$ F(\\theta) = \\sum_tp_t^Tlog(y_t) + \\left \\| d_T \\right \\| + \\sum_{t=0}^{T-1}\\eta \\xi^{\\left \\| d_t - d_{t-1} \\right \\|} $$ The loss is a combination of the reconstruction loss, the nrom of the last meaning represenation cell state (should be 0) and the average difference between two consecutive cell states (make sure that the cell does not get consumed to quickly).  \n",
    "- Models. Finally two models are defined. \n",
    "    - The training model, which outputs the three losses.\n",
    "    - The test model, which outputs the indices of the characters in the generated utterance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from keras.layers import Lambda, Embedding, Input, concatenate, ZeroPadding1D\n",
    "from src.architectures.custom_layers.sem_recurrent import SC_LSTM\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.models import Model\n",
    "\n",
    "class SCLSTM(object):\n",
    "    def __init__(self, syntactic_manipulation, vocabulary, sample_out_size):\n",
    "        self.optimizer = Adadelta(lr=1, epsilon=1e-8, rho=0.95, decay=0.0001, clipnorm=10)\n",
    "        self.vocabuary = vocabulary\n",
    "        self.sample_out_size = sample_out_size\n",
    "        \n",
    "        self.build_model(syntactic_manipulation)\n",
    "    \n",
    "    def build_model(self, syntactic_manipulation):\n",
    "        #max_sentence_len\n",
    "        #we need 3 extra classes for the dummy-char, unk-char and one for start-char\n",
    "        nclasses = len(self.vocabuary) + 3\n",
    "        lstm_units = 1024\n",
    "        dropout_word_idx = max(self.vocabuary.values()) + 1\n",
    "        \n",
    "        # == == == == == #\n",
    "        # Define Inputs  #\n",
    "        # == == == == == #\n",
    "        \n",
    "        semantic_inputs = []\n",
    "        for attribute in sorted(list(vector_lengts.keys())):\n",
    "            vec_len = vector_lengts[attribute]\n",
    "            attr_idx = Input(batch_shape=(None, vec_len), dtype='float32', name='{}_idx'.format(attribute.replace(' ', '_')))\n",
    "            semantic_inputs.append(attr_idx)\n",
    "        \n",
    "        if syntactic_manipulation:\n",
    "            attr_idx = Input(batch_shape=(None, utt_fw_input_dimension), dtype='float32', name='{}_idx'.format('utt_fw'))\n",
    "            inputs = semantic_inputs + [attr_idx]\n",
    "        else:\n",
    "            inputs = semantic_inputs\n",
    "        \n",
    "        meaning_representation = concatenate(inputs=inputs)\n",
    "        \n",
    "        # == == == == == #\n",
    "        # Define Outputs #\n",
    "        # == == == == == #\n",
    "        \n",
    "        output_idx = Input(batch_shape=(None, self.sample_out_size), dtype='int32', name='character_output')\n",
    "        \n",
    "        #we just represent characters as a one-hot encoded vectors. This makes the computation of the reconstruciton loss easier. \n",
    "        one_hot_weights = np.identity(nclasses)\n",
    "\n",
    "        one_hot_out_embeddings = Embedding(\n",
    "            input_length=self.sample_out_size,\n",
    "            input_dim=nclasses,\n",
    "            output_dim=nclasses,\n",
    "            weights=[one_hot_weights],\n",
    "            trainable=False,\n",
    "            name='one_hot_out_embeddings'\n",
    "        )\n",
    "        \n",
    "        #dimensions: batch_size x max_sentence_len x nclasses\n",
    "        output_one_hot_embeddings = one_hot_out_embeddings(output_idx)\n",
    "        \n",
    "        # == == == == == ==#\n",
    "        # Define Generator #\n",
    "        # == == == == == ==#\n",
    "        \n",
    "        #Step 1: Preprend a start-vector to the inputs, since the generation of the next character is conditioned on the previous.\n",
    "        #Thus, we need to shift the inputs by one. w_i ~ P(w_i | w_(i-1), ... , w_0, d_(i-1))\n",
    "        padding = ZeroPadding1D(padding=(1, 0))(output_one_hot_embeddings)\n",
    "        previous_char_slice = Lambda(lambda x: x[:, :-1,:], output_shape=(self.sample_out_size, nclasses))(padding)\n",
    "        \n",
    "        #Step 2: Define the recurrent layer. \n",
    "        lstm = SC_LSTM(\n",
    "            lstm_units,\n",
    "            nclasses,\n",
    "            softmax_temperature=None,\n",
    "            return_da=True,\n",
    "            return_state=False,\n",
    "            use_bias=True,\n",
    "            return_sequences=True,\n",
    "            implementation=2,\n",
    "            dropout=0.2,\n",
    "            recurrent_dropout=0.2,\n",
    "            sc_dropout=0.2\n",
    "        )\n",
    "        \n",
    "        generated_output, da_t, da_history = lstm([previous_char_slice, meaning_representation])\n",
    "        \n",
    "        # == == == == ==#\n",
    "        # Define Losses #\n",
    "        # == == == == ==#\n",
    "        \n",
    "        #Reconstruction Loss\n",
    "        def vae_cross_ent_loss(args):\n",
    "            x_truth, x_decoded_final = args\n",
    "            x_truth_flatten = K.reshape(x_truth, shape=(-1, K.shape(x_truth)[-1]))\n",
    "            x_decoded_flat = K.reshape(x_decoded_final, shape=(-1, K.shape(x_decoded_final)[-1]))\n",
    "            cross_ent = K.categorical_crossentropy(x_truth_flatten, x_decoded_flat)\n",
    "            cross_ent = K.reshape(cross_ent, shape=(-1, K.shape(x_truth)[1]))\n",
    "            sum_over_sentences = K.sum(cross_ent, axis=1)\n",
    "            return sum_over_sentences\n",
    "        \n",
    "        #Make sure the MR vector converges to 0, i.e. all the attributes have been consumend\n",
    "        def da_loss_fun(args):\n",
    "            da = args[0]\n",
    "            return K.l2_normalize(da, axis=1)\n",
    "        \n",
    "        #Make sure the changes in the MR vector are not too large\n",
    "        def da_history_loss_fun(args):\n",
    "            da_t = args[0]\n",
    "            zeta = 100\n",
    "            n = 1e-4\n",
    "            # shape: batch_size, sample_size\n",
    "            norm_of_differnece = K.sum(n*zeta**K.l2_normalize(da_t[:, 1:, :] - da_t[:, :-1, :], axis=1), axis=2)\n",
    "            n1 =norm_of_differnece\n",
    "            return K.sum(n1, axis=1)\n",
    "        \n",
    "        main_loss = Lambda(vae_cross_ent_loss, output_shape=(1,), name='main')([output_one_hot_embeddings, generated_output])\n",
    "        da_loss = Lambda(da_loss_fun, output_shape=(1,), name='dialogue_act')([da_t])\n",
    "        da_history_loss = Lambda(da_history_loss_fun, output_shape=(1,), name='dialogue_history')([da_history])\n",
    "        \n",
    "        self.train_model = Model(inputs=inputs + [output_idx], outputs=[main_loss, da_loss, da_history_loss])\n",
    "        \n",
    "        # == == == == == #\n",
    "        # Define Outputs #\n",
    "        # == == == == == #\n",
    "        \n",
    "        argmax = Lambda(lambda x: K.argmax(x, axis=2), output_shape=(self.sample_out_size,))(generated_output)\n",
    "        self.test_model = Model(inputs=inputs + [output_idx], outputs=argmax)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start the training procedure, we need to make sure, that we can generate some outputs during the training phase. This is important because the losses alone are not enough to get a good grasp of the performance. Thus, we exploit some nice properties of Keras: custom callbacks. \n",
    "Callbacks are funcions, which can be passed to the training procedure. You can decide when a particular function should be executed: begin or end of epoch, begin or end of batch, begin or end of training. We want to output the predictions for the test-set at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "\n",
    "class OutputCallback(Callback):\n",
    "    def __init__(self, test_model, model_input, da_acts, lex_dict, delex_vocab, char_vocab, sampled_features=-1, delimiter='', fname='../../logging/test_output'):\n",
    "        self.model_input = model_input\n",
    "        self.char_vocab = char_vocab\n",
    "        self.test_model = test_model\n",
    "        self.delimiter = delimiter\n",
    "        self.fname = fname\n",
    "        self.lex_dict = lex_dict\n",
    "        self.da_acts = da_acts\n",
    "        self.delex_vocab = delex_vocab\n",
    "        self.sampled_features = sampled_features\n",
    "\n",
    "        super(OutputCallback, self).__init__()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        ofile = open('{}_{}.txt'.format(self.fname, str(epoch)), 'wt', encoding='utf-8')\n",
    "        inverse_vocab = {v: k for (k, v) in self.char_vocab.items()} # idx -> char mapping\n",
    "        \n",
    "        #output is a matrix of size: number of test samples x sentence length -> each row is one generated utterance\n",
    "        predictions = self.test_model.predict(self.model_input, batch_size=1024, verbose=1)\n",
    "        \n",
    "        sen_dict = []\n",
    "        idx = -1\n",
    "        for i, sentence in enumerate(predictions):\n",
    "            if i % self.sampled_features == 0 or self.sampled_features == -1:\n",
    "                idx += 1\n",
    "            \n",
    "            #translate each utterance from a list of indices to characters\n",
    "            list_txt_idx = [int(x) for x in sentence.tolist()]\n",
    "            txt_list = [inverse_vocab.get(int(x), '') for x in list_txt_idx]\n",
    "            oline = self.delimiter.join(txt_list)\n",
    "            #replace the placeholders with their respective values (e.g. XNAME -> Blue Spice)\n",
    "            for lex_key, val in self.lex_dict.items():\n",
    "                original_token = self.delex_vocab[lex_key][idx]\n",
    "                oline = oline.replace(val, original_token)\n",
    "            sen_dict.append(oline)\n",
    "            \n",
    "        \n",
    "        #write file   \n",
    "        for sentence in sen_dict:\n",
    "            ofile.write('{}\\n'.format(sentence))\n",
    "        ofile.write('\\n')\n",
    "        ofile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to prepare the input to the neural network. Keras fit function needs two types of inputs:\n",
    "- The inputs for the nerual network, which in our case are the attribute vectors, the vector for the syntactic manipulation and the reference utterance.\n",
    "- The outputs of the neural network, which are usually used to compute the loss. However, we compute the loss directly in our neural network, thus, we just use three dummy vectors - one for each loss component.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The delexicalized fileds:\n",
      "{'food': 'XFOODX',\n",
      " 'name': 'XNAMEX',\n",
      " 'near': 'XNESRX'}\n",
      "Example of Delex Vocabulary: \n",
      "\t- Name: The Cricketers\n",
      "\t- Near: Avalon\n",
      "\t- Food: \n"
     ]
    }
   ],
   "source": [
    "from src.data_processing.delexicalise_data import _get_delex_fields\n",
    "from collections import defaultdict\n",
    "model_object = SCLSTM(False, char_vocab, max_sentence_len)\n",
    "\n",
    "def _prepare_input(mr_vecs, idx_data, surface_features):\n",
    "    sem_input = sorted(list(mr_vecs.items()), key=lambda x: x[0])\n",
    "    sem_input = [x[1] for x in sem_input]\n",
    "    \n",
    "    if surface_features is not None:\n",
    "        input_data = sem_input + [surface_features]\n",
    "    else:\n",
    "        input_data = sem_input\n",
    "    \n",
    "    input_data += [idx_data]\n",
    "    output_data = [np.ones(len(input_data[0]))] * 3\n",
    "    \n",
    "    return input_data, output_data\n",
    "\n",
    "def _get_lexicalize_dict(parsed_mrs, delex_fields):\n",
    "    \"\"\"\n",
    "    Helper function, which creates a mapping for each delexicalized attribute to the correct value. \n",
    "    This is done to replace the placeholders by the correct value at the end.\n",
    "    \"\"\"\n",
    "    delex_vocabulary = defaultdict(lambda: [])\n",
    "    for attribute, replacement_token in delex_fields.items():\n",
    "        values = [x.get(attribute, '') for x in parsed_mrs]\n",
    "        delex_vocabulary[attribute] = values\n",
    "    return delex_vocabulary\n",
    "\n",
    "#Let's first train without the syntacitc manipulaiton first\n",
    "train_input, train_output =  _prepare_input(train_mr_vecs, train_idx_data, None)\n",
    "valid_input, valid_output =  _prepare_input(valid_mr_vecs, valid_idx_data, None)\n",
    "test_input, _ = _prepare_input(test_vectorised_mrs_only, test_mr_only_dummy_idx, None)\n",
    "\n",
    "delex_fields = _get_delex_fields(attribute_tokens, delex_attributes)\n",
    "print('The delexicalized fileds:')\n",
    "pp.pprint(delex_fields)\n",
    "\n",
    "test_delex_vocab = _get_lexicalize_dict(test_process_mr_only, delex_fields)\n",
    "print('Example of Delex Vocabulary: ')\n",
    "print('\\t- Name:', test_delex_vocab['name'][idx])\n",
    "print('\\t- Near:', test_delex_vocab['near'][idx])\n",
    "print('\\t- Food:', test_delex_vocab['food'][idx])\n",
    "\n",
    "\n",
    "output_callback = OutputCallback(\n",
    "    model_object.test_model, \n",
    "    test_input, \n",
    "    test_process_mr_only, \n",
    "    delex_fields, \n",
    "    test_delex_vocab, \n",
    "    char_vocab\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Train (finally)\n",
    "Prepare your GPUs because now train our model. We use the Adadelta optimizer, choose a batch_size of 256, and train for about 30 epochs. The OutputCallback will store the outputs in a file located in the \"e2e_nlg/logging\" folder (which you need to create in case you haven't). In case your GPU has not enough memory, try to reduce the batch-size first, then reduce the size of the LSTM. \n",
    "\n",
    "First, we just train the vanilla SC-LSTM without the syntactic conditioning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42061 samples, validate on 4672 samples\n",
      "630/630 [==============================] - 4s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "model_object.train_model.compile(optimizer=model_object.optimizer, loss=lambda y_true, y_pred: y_pred)\n",
    "model_object.train_model.fit(\n",
    "    x=train_input,\n",
    "    y=train_output,\n",
    "    epochs=30,\n",
    "    batch_size=256,\n",
    "    validation_data=(valid_input, valid_output),\n",
    "    callbacks=[output_callback]\n",
    ")\n",
    "output_callback.on_epoch_end('final')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at a few examples. We load the outputs of the last epoch and print a few utterances at random. \n",
    "\n",
    "We see that the neural network relies on the most common sentence structure. After reading 20 of these utterances it becomes clear, why they need a bit of variety. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name[The Wrestlers], eatType[pub], food[Italian], priceRange[moderate], area[city centre], familyFriendly[no], near[Raja Indian Cuisine]\n",
      "The Wrestlers is a pub that serves Italian food in the moderate price range. It is located in the city centre near Raja Indian Cuisine. It is not kid friendly.\n",
      "\n",
      "name[The Phoenix], eatType[pub], food[French], priceRange[cheap], customer rating[5 out of 5], area[riverside], familyFriendly[no], near[Crowne Plaza Hotel]\n",
      "The Phoenix is a cheap pub that serves French food and is located in the riverside area near Crowne Plaza Hotel. It is not family-friendly.\n",
      "\n",
      "name[The Phoenix], eatType[restaurant], food[Fast food], priceRange[more than £30], area[riverside], familyFriendly[yes], near[Raja Indian Cuisine]\n",
      "The Phoenix is a restaurant that serves Fast food food in the moderate price range. It is located in the riverside area near Raja Indian Cuisine.\n",
      "\n",
      "name[The Cricketers], eatType[coffee shop], customer rating[low], familyFriendly[no], near[Ranch]\n",
      "The Cricketers is a coffee shop near Ranch with a low customer rating and is not family-friendly.\n",
      "\n",
      "name[The Vaults], eatType[pub], food[French], priceRange[more than £30], area[riverside], familyFriendly[no], near[Raja Indian Cuisine]\n",
      "The Vaults is a pub that serves French food in the £20-25 price range. It is located in the riverside area near Raja Indian Cuisine.\n",
      "\n",
      "name[The Wrestlers], eatType[restaurant], food[Italian], priceRange[more than £30], area[riverside], familyFriendly[yes], near[Raja Indian Cuisine]\n",
      "The Wrestlers is a restaurant that serves Italian food in the moderate price range. It is located in the riverside area near Raja Indian Cuisine.\n",
      "\n",
      "name[The Phoenix], eatType[restaurant], food[Fast food], priceRange[less than £20], area[riverside], familyFriendly[yes], near[Raja Indian Cuisine]\n",
      "The Phoenix is a family friendly restaurant located near Raja Indian Cuisine.\n",
      "\n",
      "name[The Phoenix], eatType[pub], food[French], priceRange[less than £20], customer rating[average], area[riverside], familyFriendly[no], near[Crowne Plaza Hotel]\n",
      "The Phoenix is a pub that serves French food in the less than £20 price range. It is located in the riverside area near Crowne Plaza Hotel. It is not family-friendly.\n",
      "\n",
      "name[Cocum], eatType[pub], near[The Sorrento]\n",
      "Cocum is a pub near The Sorrento with a price range of more than £30.\n",
      "\n",
      "name[The Cricketers], eatType[restaurant], customer rating[average], familyFriendly[no], near[Crowne Plaza Hotel]\n",
      "The Cricketers is a restaurant with a low customer rating. It is not family-friendly.\n",
      "\n",
      "name[The Mill], eatType[pub], food[Fast food], priceRange[moderate], customer rating[1 out of 5], area[city centre], familyFriendly[no], near[Café Sicilia]\n",
      "The Mill is a moderately priced Fast food pub located in the city centre near Café Sicilia. It is not kid friendly.\n",
      "\n",
      "name[The Wrestlers], eatType[restaurant], food[Italian], priceRange[moderate], area[riverside], familyFriendly[no], near[Raja Indian Cuisine]\n",
      "The Wrestlers is a moderately priced restaurant located near Raja Indian Cuisine.\n",
      "\n",
      "name[The Cricketers], eatType[coffee shop], customer rating[3 out of 5], familyFriendly[yes], near[Crowne Plaza Hotel]\n",
      "The Cricketers is a coffee shop with a customer rating of 3 out of 5 and is kid friendly. It is located near Crowne Plaza Hotel.\n",
      "\n",
      "name[The Vaults], eatType[pub], food[Italian], priceRange[high], customer rating[average], area[riverside], familyFriendly[no], near[Rainbow Vegetarian Café]\n",
      "The Vaults is a pub that serves Italian food in the high price range. It is located in the riverside area near Rainbow Vegetarian Café. It has a low customer rating.\n",
      "\n",
      "name[Cocum], eatType[pub], customer rating[low], near[Express by Holiday Inn]\n",
      "Cocum is a pub with a low customer rating. It is located near Express by Holiday Inn.\n",
      "\n",
      "name[The Phoenix], eatType[pub], food[Fast food], priceRange[cheap], area[riverside], familyFriendly[yes], near[Raja Indian Cuisine]\n",
      "The Phoenix is a child friendly pub that serves Fast food food in the less than £20 price range. It is located in the riverside area near Raja Indian Cuisine.\n",
      "\n",
      "name[The Waterman], eatType[restaurant], food[Indian], priceRange[less than £20], area[riverside], familyFriendly[no], near[Raja Indian Cuisine]\n",
      "The Waterman is a restaurant that serves Indian food in the less than £20 price range. It is located in the riverside area near Raja Indian Cuisine.\n",
      "\n",
      "name[The Wrestlers], eatType[restaurant], food[Japanese], priceRange[less than £20], area[city centre], familyFriendly[no], near[Raja Indian Cuisine]\n",
      "The Wrestlers is a Japanese restaurant located in the city centre near Raja Indian Cuisine. It is not family-friendly and has a price range of less than £20.\n",
      "\n",
      "name[Blue Spice], eatType[restaurant], food[English], area[riverside], familyFriendly[yes], near[Rainbow Vegetarian Café]\n",
      "Blue Spice is a family friendly restaurant located near Rainbow Vegetarian Café.\n",
      "\n",
      "name[The Cricketers], eatType[restaurant], food[English], priceRange[high], customer rating[average], area[riverside], familyFriendly[no], near[Café Rouge]\n",
      "The Cricketers is a high priced restaurant that serves English food. It is not family-friendly and has a low customer rating.\n",
      "\n",
      "name[Giraffe], eatType[restaurant], food[French], area[city centre], familyFriendly[yes], near[Raja Indian Cuisine]\n",
      "Giraffe is a family friendly French restaurant located in the city centre.\n",
      "\n",
      "name[The Cricketers], eatType[coffee shop], customer rating[low], familyFriendly[no], near[Express by Holiday Inn]\n",
      "The Cricketers is a coffee shop near Express by Holiday Inn with a low customer rating and is not family-friendly.\n",
      "\n",
      "name[The Cricketers], eatType[restaurant], food[Chinese], priceRange[£20-25], customer rating[high], area[riverside], familyFriendly[yes], near[All Bar One]\n",
      "The Cricketers is a restaurant that serves Chinese food in the £20-25 price range. It is located in the riverside area near All Bar One. It is kid friendly and has a customer rating of 3 out of 5.\n",
      "\n",
      "name[The Vaults], eatType[pub], food[Italian], priceRange[less than £20], customer rating[low], area[riverside], familyFriendly[yes], near[Rainbow Vegetarian Café]\n",
      "The Vaults is a family friendly pub located near Rainbow Vegetarian Café in the riverside area.  It serves Italian food and has a low customer rating.\n",
      "\n",
      "name[The Punter], eatType[pub], food[English], priceRange[high], area[riverside], familyFriendly[no], near[Raja Indian Cuisine]\n",
      "The Punter is a pub that serves English food in the high price range. It is located in the riverside area near Raja Indian Cuisine.\n",
      "\n",
      "name[The Punter], eatType[restaurant], food[Italian], priceRange[£20-25], customer rating[high], area[city centre], familyFriendly[no], near[Express by Holiday Inn]\n",
      "The Punter is a Italian restaurant with a high price range and is located in the city centre. It is near Express by Holiday Inn. Its customer rating is 1 out of 5.\n",
      "\n",
      "name[The Waterman], eatType[restaurant], food[Italian], priceRange[less than £20], area[city centre], familyFriendly[no], near[Raja Indian Cuisine]\n",
      "The Waterman is a Italian restaurant located in the city centre near Raja Indian Cuisine. It is not family-friendly and has a price range of less than £20.\n",
      "\n",
      "name[The Phoenix], eatType[pub], food[French], priceRange[less than £20], customer rating[low], area[city centre], familyFriendly[yes], near[Crowne Plaza Hotel]\n",
      "The Phoenix is a family-friendly pub located near Crowne Plaza Hotel in the city centre.  It serves French food and has a low customer rating.\n",
      "\n",
      "name[The Mill], eatType[pub], food[Fast food], priceRange[high], customer rating[average], area[riverside], familyFriendly[no], near[Café Sicilia]\n",
      "The Mill is a pub that serves Fast food food in the high price range. It is located in the riverside area near Café Sicilia. It has a low customer rating.\n",
      "\n",
      "name[The Mill], eatType[pub], food[Fast food], priceRange[moderate], customer rating[3 out of 5], area[riverside], familyFriendly[no], near[Café Sicilia]\n",
      "The Mill is a moderately priced pub that serves Fast food food in the riverside area near Café Sicilia. It is not kid friendly.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "ifile = open('{}_{}.txt'.format(output_callback.fname, str('11')), 'rt', encoding='utf-8')\n",
    "output = ifile.readlines()\n",
    "for mr, utt in random.sample(list(zip(test_mr_only_raw, output)), k=30):\n",
    "    print(mr)\n",
    "    print(utt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bring the Variety "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recreate the train data but this time inlcude the first word vectors.\n",
    "model_object_var = SCLSTM(True, char_vocab, max_sentence_len)\n",
    "\n",
    "def _prepare_test_input(mr_vecs, idx_data, surface_features):\n",
    "    sem_input = sorted(list(mr_vecs.items()), key=lambda x: x[0])\n",
    "    sem_input = [x[1] for x in sem_input]\n",
    "    \n",
    "    input_samples = []\n",
    "    for *isem, isamples in zip(*sem_input, surface_features):\n",
    "        for sample in isamples:\n",
    "            input_sample = isem + [sample] + [np.zeros(shape=(max_sentence_len, 1))]\n",
    "            input_samples.append(input_sample)\n",
    "    input_samples = list(map(list, zip(*input_samples)))\n",
    "    input_samples = [np.squeeze(np.array(x), axis=-1) if x[0].shape[-1] == 1 else np.array(x) for x in input_samples]     \n",
    "    \n",
    "    return input_samples\n",
    "\n",
    "\n",
    "train_input, train_output =  _prepare_input(train_mr_vecs, train_idx_data, train_utt_fw)\n",
    "valid_input, valid_output =  _prepare_input(valid_mr_vecs, valid_idx_data, valid_utt_fw)\n",
    "test_input= _prepare_test_input(test_vectorised_mrs_only, test_mr_only_dummy_idx, first_word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42061 samples, validate on 4672 samples\n",
      "Epoch 1/30\n",
      "42061/42061 [==============================] - 466s 11ms/step - loss: 530.2800 - main_loss: 528.9825 - dialogue_act_loss: 0.0000e+00 - dialogue_history_loss: 1.2975 - val_loss: 702.7914 - val_main_loss: 701.4940 - val_dialogue_act_loss: 0.0000e+00 - val_dialogue_history_loss: 1.2974\n",
      "6300/6300 [==============================] - 18s 3ms/step\n",
      "Epoch 2/30\n",
      " 1024/42061 [..............................] - ETA: 7:13 - loss: 420.6974 - main_loss: 419.4000 - dialogue_act_loss: 0.0000e+00 - dialogue_history_loss: 1.2975"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-5b286bd97e09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moutput_callback_var\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     21\u001b[0m \u001b[0moutput_callback_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'final'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Jan\\Anaconda3\\envs\\keras_environment\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\Users\\Jan\\Anaconda3\\envs\\keras_environment\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Jan\\Anaconda3\\envs\\keras_environment\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Jan\\Anaconda3\\envs\\keras_environment\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m                                 session)\n\u001b[1;32m-> 2636\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Jan\\Anaconda3\\envs\\keras_environment\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output_callback_var = OutputCallback(\n",
    "    model_object_var.test_model, \n",
    "    test_input, \n",
    "    test_process_mr_only, \n",
    "    delex_fields, \n",
    "    test_delex_vocab, \n",
    "    char_vocab,\n",
    "    sampled_features=10\n",
    "    \n",
    ")\n",
    "\n",
    "model_object_var.train_model.compile(optimizer=model_object.optimizer, loss=lambda y_true, y_pred: y_pred)\n",
    "model_object_var.train_model.fit(\n",
    "    x=train_input,\n",
    "    y=train_output,\n",
    "    epochs=30,\n",
    "    batch_size=256,\n",
    "    validation_data=(valid_input, valid_output),\n",
    "    callbacks=[output_callback_var]\n",
    ")\n",
    "output_callback_var.on_epoch_end('final')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name[The Punter], eatType[restaurant], food[Chinese], priceRange[moderate], area[city centre], familyFriendly[no], near[Raja Indian Cuisine]\n",
      "The Punter is a moderately priced Chinese restaurant located in the city centre. It is not kid friendly.\n",
      "\n",
      "name[The Cricketers], eatType[restaurant], customer rating[3 out of 5], familyFriendly[yes], near[Avalon]\n",
      "The Cricketers is a kid friendly restaurant with a customer rating of 3 out of 5.\n",
      "\n",
      "name[The Phoenix], eatType[pub], food[French], priceRange[cheap], customer rating[5 out of 5], area[city centre], familyFriendly[no], near[Crowne Plaza Hotel]\n",
      "The Phoenix is a cheap pub that serves French food in the city centre near Crowne Plaza Hotel. It has a customer rating of 5 out of 5.\n",
      "\n",
      "name[The Phoenix], eatType[restaurant], food[Fast food], priceRange[less than £20], area[city centre], familyFriendly[no], near[Raja Indian Cuisine]\n",
      "The Phoenix is a Fast food restaurant located in the city centre near Raja Indian Cuisine. It is not family-friendly and has a price range of less than £20.\n",
      "\n",
      "name[The Phoenix], eatType[restaurant], food[Fast food], priceRange[less than £20], area[riverside], familyFriendly[no], near[Raja Indian Cuisine]\n",
      "The Phoenix is a restaurant that serves Fast food food in the less than £20 price range. It is located in the riverside area near Raja Indian Cuisine.\n",
      "\n",
      "name[The Punter], eatType[restaurant], food[Indian], priceRange[more than £30], customer rating[high], area[riverside], familyFriendly[no], near[Express by Holiday Inn]\n",
      "The Punter is a high priced restaurant that serves Indian food. It is located in the riverside area near Express by Holiday Inn. It is not kid friendly.\n",
      "\n",
      "name[The Cricketers], eatType[restaurant], food[Chinese], priceRange[moderate], customer rating[1 out of 5], area[city centre], familyFriendly[no], near[All Bar One]\n",
      "The Cricketers is a moderately priced Chinese restaurant located in the city centre near All Bar One. It is not kid friendly.\n",
      "\n",
      "name[The Vaults], eatType[restaurant], food[French], priceRange[less than £20], area[riverside], familyFriendly[no], near[Raja Indian Cuisine]\n",
      "The Vaults is a restaurant that serves French food in the less than £20 price range. It is located in the riverside area near Raja Indian Cuisine.\n",
      "\n",
      "name[The Vaults], eatType[restaurant], food[Indian], priceRange[cheap], area[riverside], familyFriendly[yes], near[Raja Indian Cuisine]\n",
      "The Vaults is a cheap family friendly restaurant located near Raja Indian Cuisine.\n",
      "\n",
      "name[The Wrestlers], eatType[restaurant], food[Italian], priceRange[more than £30], area[riverside], familyFriendly[yes], near[Raja Indian Cuisine]\n",
      "The Wrestlers is a restaurant that serves Italian food in the moderate price range. It is located in the riverside area near Raja Indian Cuisine.\n",
      "\n",
      "name[The Vaults], eatType[pub], food[Japanese], priceRange[high], customer rating[3 out of 5], area[riverside], familyFriendly[no], near[Raja Indian Cuisine]\n",
      "The Vaults is a pub that serves Japanese food in the £20-25 price range. It is located in the riverside area near Raja Indian Cuisine. It has a high customer rating.\n",
      "\n",
      "name[Cocum], eatType[pub], customer rating[low], near[Café Sicilia]\n",
      "Cocum is a pub with a low customer rating. It is located near Café Sicilia.\n",
      "\n",
      "name[Strada], eatType[pub], customer rating[1 out of 5], near[All Bar One]\n",
      "Strada is a pub with a customer rating of 1 out of 5. It is located near All Bar One.\n",
      "\n",
      "name[The Mill], eatType[restaurant], food[English], priceRange[moderate], customer rating[1 out of 5], area[city centre], familyFriendly[yes], near[Café Rouge]\n",
      "The Mill is a moderately priced English restaurant located in the city centre near Café Rouge. It is kid friendly and has a customer rating of 1 out of 5.\n",
      "\n",
      "name[The Vaults], eatType[pub], food[Japanese], priceRange[moderate], customer rating[1 out of 5], area[city centre], familyFriendly[no], near[Raja Indian Cuisine]\n",
      "The Vaults is a moderately priced Japanese pub located in the city centre near Raja Indian Cuisine. It is not kid friendly.\n",
      "\n",
      "name[The Phoenix], eatType[pub], food[French], priceRange[£20-25], area[riverside], familyFriendly[yes], near[Raja Indian Cuisine]\n",
      "The Phoenix is a pub that serves French food in the moderate price range. It is located in the riverside area near Raja Indian Cuisine.\n",
      "\n",
      "name[The Punter], eatType[restaurant], food[Indian], priceRange[more than £30], customer rating[high], area[riverside], familyFriendly[yes], near[Express by Holiday Inn]\n",
      "The Punter is a high priced restaurant that is family friendly and is located near Express by Holiday Inn.\n",
      "\n",
      "name[The Vaults], eatType[restaurant], food[French], priceRange[moderate], area[riverside], familyFriendly[yes], near[Raja Indian Cuisine]\n",
      "The Vaults is a kid friendly restaurant that serves French food in the moderate price range. It is located in the riverside area near Raja Indian Cuisine.\n",
      "\n",
      "name[The Mill], eatType[restaurant], food[English], priceRange[moderate], customer rating[1 out of 5], area[riverside], familyFriendly[yes], near[Café Rouge]\n",
      "The Mill is a moderately priced restaurant that is family friendly and is located near Café Rouge.\n",
      "\n",
      "name[The Wrestlers], eatType[restaurant], food[Japanese], priceRange[moderate], area[city centre], familyFriendly[no], near[Raja Indian Cuisine]\n",
      "The Wrestlers is a moderately priced Japanese restaurant located in the city centre. It is not kid friendly.\n",
      "\n",
      "name[Giraffe], eatType[pub], food[English], area[riverside], familyFriendly[yes], near[Rainbow Vegetarian Café]\n",
      "Giraffe is a family friendly pub serving English food in the riverside area near Rainbow Vegetarian Café.\n",
      "\n",
      "name[The Punter], eatType[pub], food[Chinese], priceRange[less than £20], area[riverside], familyFriendly[no], near[Raja Indian Cuisine]\n",
      "The Punter is a pub that serves Chinese food in the less than £20 price range. It is located in the riverside area near Raja Indian Cuisine.\n",
      "\n",
      "name[The Wrestlers], eatType[pub], food[Japanese], priceRange[high], area[riverside], familyFriendly[yes], near[Raja Indian Cuisine]\n",
      "The Wrestlers is a pub that serves Japanese food in the high price range. It is located in the riverside area near Raja Indian Cuisine.\n",
      "\n",
      "name[Strada], eatType[pub], customer rating[1 out of 5], near[Rainbow Vegetarian Café]\n",
      "Strada is a pub with a customer rating of 1 out of 5. It is located near Rainbow Vegetarian Café.\n",
      "\n",
      "name[The Waterman], eatType[pub], food[Indian], priceRange[high], area[city centre], familyFriendly[yes], near[Raja Indian Cuisine]\n",
      "The Waterman is a pub that serves Indian food in the city centre near Raja Indian Cuisine. It is child friendly and has a price range of more than £30.\n",
      "\n",
      "name[The Cricketers], eatType[restaurant], food[Chinese], priceRange[cheap], customer rating[average], area[city centre], familyFriendly[yes], near[All Bar One]\n",
      "The Cricketers is a cheap Chinese restaurant located in the city centre near All Bar One. It is family-friendly and has a customer rating of 5 out of 5.\n",
      "\n",
      "name[The Punter], eatType[restaurant], food[Italian], priceRange[high], customer rating[1 out of 5], area[city centre], familyFriendly[no], near[Rainbow Vegetarian Café]\n",
      "The Punter is a Italian restaurant in the city centre near Rainbow Vegetarian Café. It has a high price range and a customer rating of 1 out of 5.\n",
      "\n",
      "name[Clowns], eatType[pub], customer rating[3 out of 5], near[All Bar One]\n",
      "Clowns is a pub near All Bar One with a customer rating of 3 out of 5.\n",
      "\n",
      "name[The Mill], eatType[restaurant], food[English], priceRange[more than £30], customer rating[high], area[city centre], familyFriendly[yes], near[Café Rouge]\n",
      "The Mill is a high priced restaurant that is family friendly and is located in the city centre.\n",
      "\n",
      "name[The Wrestlers], eatType[pub], food[Japanese], priceRange[less than £20], area[riverside], familyFriendly[yes], near[Raja Indian Cuisine]\n",
      "The Wrestlers is a family friendly pub located near Raja Indian Cuisine in the riverside area.  It serves Japanese food and is located near Raja Indian Cuisine.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "ifile = open('{}_{}.txt'.format(output_callback.fname, str('11')), 'rt', encoding='utf-8')\n",
    "output = ifile.readlines()\n",
    "for mr, utt in random.sample(list(zip(test_mr_only_raw, output)), k=30):\n",
    "    print(mr)\n",
    "    print(utt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras_environment]",
   "language": "python",
   "name": "conda-env-keras_environment-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
